# Model configuration
model_name_or_path: "Qwen/Qwen2.5-0.5B-Instruct"
# model_name_or_path: "llm-jp/llm-jp-3-1.8b-instruct2"

# Dataset configuration
train_dataset_path: "data/debug_train.jsonl"
eval_dataset_path: "data/debug_eval.jsonl"

# LoRA configuration
use_lora: true  # Set to false to disable LoRA
lora_config:
  r: 16  # LoRA attention dimension
  lora_alpha: 32  # Alpha parameter for LoRA scaling
  lora_dropout: 0.05  # Dropout probability for LoRA layers
  bias: "none"  # Bias type for LoRA
  task_type: "CAUSAL_LM"  # Task type for LoRA
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "up_proj", "down_proj", "gate_proj"]  # Which modules to apply LoRA to

# Quantization configuration
use_quantization: true  # Set to false to disable quantization
quantization_config:
  bits: 4  # Quantization precision: 4 or 8 bits
  load_in_4bit: true  # Will be set based on bits value
  load_in_8bit: false  # Will be set based on bits value
  llm_int8_threshold: 6.0  # For 8-bit quantization
  llm_int8_has_fp16_weight: false  # For 8-bit quantization
  bnb_4bit_compute_dtype: "float16"  # For 4-bit quantization
  bnb_4bit_use_double_quant: true  # For 4-bit quantization
  bnb_4bit_quant_type: "nf4"  # For 4-bit quantization

# Training configuration
training_args:
  output_dir: "./outputs"
  eval_strategy: "epoch"
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  learning_rate: 1.0e-5
  num_train_epochs: 3.0
  lr_scheduler_type: "linear"
  warmup_ratio: 0.0
  logging_strategy: "epoch"
  save_strategy: "epoch"
  report_to: "all"
  # max_seq_length: 512
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  predict_with_generate: true
  generation_max_length: 768
  generation_num_beams: 4
  metric_for_best_model: "eval_f1"
  greater_is_better: true
  local_rank: 0

# Early stopping configuration
early_stopping:
  # enabled: true  # Set to true to enable early stopping
  early_stopping_patience: 5  # Number of epochs to wait before early stopping
  early_stopping_threshold: 0.0  # Threshold for early stopping
  # metric_for_best_model: "eval_f1"  # Use F1 score for early stopping

# Wandb configuration
wandb:
  enabled: false  # Set to false to disable wandb during debugging
  project: "tutorial2025_re"  # Project name in wandb
  name: null  # Run name (will be auto-generated if null)
  group: null  # Group name for related runs
  tags: []  # List of tags for the run
  debug: false  # Set to true for wandb debug mode

# Response template for data collator
response_template: "### 指示:\n"
